{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "directory_path_train = './mfcc_post_processing/'\n",
    "directory_path_test = './mfcc_post_processing_test/'\n",
    "file_pattern = \"*.csv\"\n",
    "csv_files_train = glob.glob(os.path.join(directory_path_train, file_pattern))\n",
    "csv_files_test = glob.glob(os.path.join(directory_path_test, file_pattern))\n",
    "dataframes_train = []\n",
    "dataframes_test = []\n",
    "\n",
    "for file in csv_files_train:\n",
    "    df = pd.read_csv(file)\n",
    "    dataframes_train.append(df)\n",
    "pd_train = pd.concat(dataframes_train, ignore_index=True)\n",
    "print(pd_train.head()) \n",
    "print(pd_train.shape)\n",
    "for file in csv_files_test:\n",
    "    df = pd.read_csv(file)\n",
    "    dataframes_test.append(df)\n",
    "pd_test = pd.concat(dataframes_test, ignore_index=True)\n",
    "print(pd_test.head())\n",
    "print(pd_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './mfcc_post_processing/1742_mfccs.csv'\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Coefficients          1          2          3          4          5  \\\n",
      "0    -558.47455   2.793246   2.786327   2.774820   2.758777   2.738251   \n",
      "1    -555.69570   6.715124   6.683780   6.631803   6.559573   6.467629   \n",
      "2    -553.84973   9.316228   9.256683   9.158134   9.021615   8.848545   \n",
      "3    -552.81780  10.768115  10.686591  10.551867  10.365642  10.130268   \n",
      "4    -552.09300  11.773888  11.635872  11.411560  11.109087  10.739025   \n",
      "\n",
      "           6         7         8         9        10        11        12  \\\n",
      "0   2.713328  2.684097  2.650674  2.613182  2.571767  2.526580  2.477794   \n",
      "1   6.356648  6.227449  6.080969  5.918281  5.740552  5.549061  5.345159   \n",
      "2   8.640726  8.400292  8.129717  7.831718  7.509308  7.165675  6.804174   \n",
      "3   9.848681  9.524376  9.161339  8.763979  8.337059  7.885606  7.414847   \n",
      "4  10.313629  9.846010  9.349272  8.835732  8.316168  7.799348  7.291704   \n",
      "\n",
      "   Instruments  \n",
      "0            0  \n",
      "1            0  \n",
      "2            3  \n",
      "3            3  \n",
      "4            3  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.drop('Instruments', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['Instruments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficients</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-558.47455</td>\n",
       "      <td>2.793246</td>\n",
       "      <td>2.786327</td>\n",
       "      <td>2.774820</td>\n",
       "      <td>2.758777</td>\n",
       "      <td>2.738251</td>\n",
       "      <td>2.713328</td>\n",
       "      <td>2.684097</td>\n",
       "      <td>2.650674</td>\n",
       "      <td>2.613182</td>\n",
       "      <td>2.571767</td>\n",
       "      <td>2.526580</td>\n",
       "      <td>2.477794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-555.69570</td>\n",
       "      <td>6.715124</td>\n",
       "      <td>6.683780</td>\n",
       "      <td>6.631803</td>\n",
       "      <td>6.559573</td>\n",
       "      <td>6.467629</td>\n",
       "      <td>6.356648</td>\n",
       "      <td>6.227449</td>\n",
       "      <td>6.080969</td>\n",
       "      <td>5.918281</td>\n",
       "      <td>5.740552</td>\n",
       "      <td>5.549061</td>\n",
       "      <td>5.345159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-553.84973</td>\n",
       "      <td>9.316228</td>\n",
       "      <td>9.256683</td>\n",
       "      <td>9.158134</td>\n",
       "      <td>9.021615</td>\n",
       "      <td>8.848545</td>\n",
       "      <td>8.640726</td>\n",
       "      <td>8.400292</td>\n",
       "      <td>8.129717</td>\n",
       "      <td>7.831718</td>\n",
       "      <td>7.509308</td>\n",
       "      <td>7.165675</td>\n",
       "      <td>6.804174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-552.81780</td>\n",
       "      <td>10.768115</td>\n",
       "      <td>10.686591</td>\n",
       "      <td>10.551867</td>\n",
       "      <td>10.365642</td>\n",
       "      <td>10.130268</td>\n",
       "      <td>9.848681</td>\n",
       "      <td>9.524376</td>\n",
       "      <td>9.161339</td>\n",
       "      <td>8.763979</td>\n",
       "      <td>8.337059</td>\n",
       "      <td>7.885606</td>\n",
       "      <td>7.414847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-552.09300</td>\n",
       "      <td>11.773888</td>\n",
       "      <td>11.635872</td>\n",
       "      <td>11.411560</td>\n",
       "      <td>11.109087</td>\n",
       "      <td>10.739025</td>\n",
       "      <td>10.313629</td>\n",
       "      <td>9.846010</td>\n",
       "      <td>9.349272</td>\n",
       "      <td>8.835732</td>\n",
       "      <td>8.316168</td>\n",
       "      <td>7.799348</td>\n",
       "      <td>7.291704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Coefficients          1          2          3          4          5  \\\n",
       "0    -558.47455   2.793246   2.786327   2.774820   2.758777   2.738251   \n",
       "1    -555.69570   6.715124   6.683780   6.631803   6.559573   6.467629   \n",
       "2    -553.84973   9.316228   9.256683   9.158134   9.021615   8.848545   \n",
       "3    -552.81780  10.768115  10.686591  10.551867  10.365642  10.130268   \n",
       "4    -552.09300  11.773888  11.635872  11.411560  11.109087  10.739025   \n",
       "\n",
       "           6         7         8         9        10        11        12  \n",
       "0   2.713328  2.684097  2.650674  2.613182  2.571767  2.526580  2.477794  \n",
       "1   6.356648  6.227449  6.080969  5.918281  5.740552  5.549061  5.345159  \n",
       "2   8.640726  8.400292  8.129717  7.831718  7.509308  7.165675  6.804174  \n",
       "3   9.848681  9.524376  9.161339  8.763979  8.337059  7.885606  7.414847  \n",
       "4  10.313629  9.846010  9.349272  8.835732  8.316168  7.799348  7.291704  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    3\n",
       "3    3\n",
       "4    3\n",
       "Name: Instruments, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79945, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "encoder = OneHotEncoder(sparse=False)\n",
    "labels_encoded = encoder.fit_transform(labels.values.reshape(-1, 1))\n",
    "print(labels_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(features, labels_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "train_features = torch.tensor(X_train.values).float()\n",
    "train_labels = torch.tensor(y_train).float()\n",
    "val_features = torch.tensor(X_val.values).float()\n",
    "val_labels = torch.tensor(y_val).float()\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(train_features, train_labels)\n",
    "val_dataset = TensorDataset(val_features, val_labels)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32  # You can adjust the batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class InstrumentClassifier(nn.Module):\n",
    "    def __init__(self, num_features, num_classes, dim_model=64, nhead=4, num_layers=2, dropout=0.1):\n",
    "        super(InstrumentClassifier, self).__init__()\n",
    "        self.embedding = nn.Linear(num_features, dim_model)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model=dim_model, nhead=nhead, dropout=dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "        self.output_layer = nn.Linear(dim_model, num_classes)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        transformer_output = self.transformer_encoder(embedded)\n",
    "        if transformer_output.dim() == 3:\n",
    "            output = self.output_layer(transformer_output[:, 0, :])  # Correct if seq_length dimension exists\n",
    "        else:\n",
    "            output = self.output_layer(transformer_output)  # Use directly if no seq_length dimension\n",
    "        return output\n",
    "\n",
    "# Create a model instance\n",
    "model = InstrumentClassifier(num_features=train_features.shape[1], num_classes=labels_encoded.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 - Train Loss: 0.0622, Val Loss: 0.0505\n",
      "Epoch 2/30 - Train Loss: 0.0526, Val Loss: 0.0504\n",
      "Epoch 3/30 - Train Loss: 0.0519, Val Loss: 0.0469\n",
      "Epoch 4/30 - Train Loss: 0.0523, Val Loss: 0.0560\n",
      "Epoch 5/30 - Train Loss: 0.0505, Val Loss: 0.0480\n",
      "Epoch 6/30 - Train Loss: 0.0490, Val Loss: 0.0484\n",
      "Epoch 7/30 - Train Loss: 0.0487, Val Loss: 0.0480\n",
      "Epoch 8/30 - Train Loss: 0.0476, Val Loss: 0.0492\n",
      "Epoch 9/30 - Train Loss: 0.0546, Val Loss: 0.0486\n",
      "Epoch 10/30 - Train Loss: 0.0502, Val Loss: 0.0468\n",
      "Epoch 11/30 - Train Loss: 0.0480, Val Loss: 0.0574\n",
      "Epoch 12/30 - Train Loss: 0.0472, Val Loss: 0.0467\n",
      "Epoch 13/30 - Train Loss: 0.0466, Val Loss: 0.0486\n",
      "Epoch 14/30 - Train Loss: 0.0460, Val Loss: 0.0538\n",
      "Epoch 15/30 - Train Loss: 0.0459, Val Loss: 0.0573\n",
      "Epoch 16/30 - Train Loss: 0.0463, Val Loss: 0.0444\n",
      "Epoch 17/30 - Train Loss: 0.0429, Val Loss: 0.0441\n",
      "Epoch 18/30 - Train Loss: 0.0416, Val Loss: 0.0396\n",
      "Epoch 19/30 - Train Loss: 0.0422, Val Loss: 0.0382\n",
      "Epoch 20/30 - Train Loss: 0.0398, Val Loss: 0.0422\n",
      "Epoch 21/30 - Train Loss: 0.0382, Val Loss: 0.0415\n",
      "Epoch 22/30 - Train Loss: 0.0391, Val Loss: 0.0372\n",
      "Epoch 23/30 - Train Loss: 0.0368, Val Loss: 0.0393\n",
      "Epoch 24/30 - Train Loss: 0.0368, Val Loss: 0.0383\n",
      "Epoch 25/30 - Train Loss: 0.0364, Val Loss: 0.0366\n",
      "Epoch 26/30 - Train Loss: 0.0370, Val Loss: 0.0342\n",
      "Epoch 27/30 - Train Loss: 0.0361, Val Loss: 0.0466\n",
      "Epoch 28/30 - Train Loss: 0.0356, Val Loss: 0.0359\n",
      "Epoch 29/30 - Train Loss: 0.0348, Val Loss: 0.0347\n",
      "Epoch 30/30 - Train Loss: 0.0335, Val Loss: 0.0330\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Set device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming the model and dataloaders are already defined and instantiated\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training and validation phases\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 30\n",
    "\n",
    "# Loop over the dataset multiple times\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss = validate_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data_path = './mfcc_post_processing/1749_mfccs.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "features = df.drop('Instruments', axis=1)\n",
    "labels = df['Instruments']\n",
    "\n",
    "# Encode labels\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "labels_encoded = encoder.fit_transform(labels.values.reshape(-1, 1))\n",
    "\n",
    "# Retrieve class names from the encoder\n",
    "class_names = [str(name) for name in encoder.categories_[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.6809\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.18      0.23     16230\n",
      "           1       0.74      0.87      0.80     43986\n",
      "\n",
      "    accuracy                           0.68     60216\n",
      "   macro avg       0.54      0.52      0.52     60216\n",
      "weighted avg       0.63      0.68      0.65     60216\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "test_features = torch.tensor(features.values).float()\n",
    "test_labels = torch.tensor(labels_encoded).float()\n",
    "\n",
    "# Create TensorDataset and DataLoader\n",
    "test_dataset = TensorDataset(test_features, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def test_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            _, labels_indices = torch.max(labels, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels_indices.cpu().numpy())\n",
    "\n",
    "    # Calculate accuracy and other classification metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    report = classification_report(all_labels, all_preds, target_names=class_names)\n",
    "    return accuracy, report\n",
    "\n",
    "# Execute the testing function\n",
    "accuracy, report = test_model(model, test_loader, device)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
